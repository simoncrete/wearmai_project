{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f35762-5836-4c9d-abb9-6e9bca5779ee",
   "metadata": {},
   "source": [
    "## Run training on WearM.AI data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "id": "3be57e11-9312-4a95-ba22-875874a4ef44",
   "metadata": {},
   "source": [
    "#### Be aware of the difference between the key point definition difference.\n",
    "First, create a wearmai preprocessing file to create a .npz file.\n",
    "Convert from WearM.AI joints to global order (SMPL) in datasets/preprocess/COCOwearmai.py\n",
    "\n",
    "WearM.AI: (19 total)\n",
    "\"keypoints\": [\"Rk_i\", \"Le\", \"Lh\", \"Lw\", \"Ra_o\", \"Ra_i\", \"Lk_o\", \"Rk_o\", \"P\", \"Rh\", \"Rs\", \"S\", \"Lk_i\", \"Rw\", \"H\", \"La_i\", \"Re\", \"La_o\", \"Ls\"],\n",
    "\n",
    "    '''\n",
    "    *0 right knee inner = Rk_i --> 2\n",
    "    1 left elbow = Le --> 11\n",
    "    2 left hip = Lh --> 4\n",
    "    3 left wrist = Lw --> 12\n",
    "    *4 right ankle outer = Ra_o --> 1\n",
    "    *5 right ankle inner = Ra_i --> 1\n",
    "    *6 left knee outer = Lk_o --> 5\n",
    "    *7 right knee outer = Rk_o --> 2\n",
    "    8 pelvis = P --> 15\n",
    "    9 right hip = Rh --> 3\n",
    "    10 right shoulder = Rs --> 9\n",
    "    11 spine = S --> 17\n",
    "    *12 left knee inner = Lk_i --> 5\n",
    "    13 right wrist = Rw --> 7\n",
    "    14 head = H --> 14\n",
    "    *15 left ankle inner = La_i --> 6\n",
    "    16 right elbow = Re --> 8\n",
    "    *17 left ankle outer = La_o --> 6\n",
    "    18 left shoulder = Ls --> 10\n",
    "\n",
    "    possible mapping with merging\n",
    "    0 right knee --> 2\n",
    "    1 left elbow --> 11\n",
    "    2 left hip --> 4\n",
    "    3 left wrist --> 12\n",
    "    4 right ankle --> 1\n",
    "    5 left knee  --> 5\n",
    "    6 pelvis  --> 15\n",
    "    7 right hip  --> 3\n",
    "    8 right shoulder --> 9\n",
    "    9 spine --> 17\n",
    "    10 right wrist --> 7\n",
    "    11 head --> 14\n",
    "    12 left ankle  --> 6\n",
    "    13 right elbow --> 8\n",
    "    14 left shoulder --> 10\n",
    "    '''\n",
    "\n",
    "###Our mapping with merging of inner and outer\n",
    "joints_idx = [1, 10, 3, 11, 0, 4, 14, 2, 8, 16, 6, 13, 5, 7, 9]\n",
    "\n",
    "COCO2014: (17 total)\n",
    "[\"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"]\n",
    "0 nose\n",
    "1 left eye\n",
    "2 right eye\n",
    "3 left ear\n",
    "4 right ear\n",
    "5 left shoulder\n",
    "6 right shoulder \n",
    "7 left elbow\n",
    "8 right elbow\n",
    "9 left wrist\n",
    "10 right wrist \n",
    "11 left hip\n",
    "12 right hip\n",
    "13 left knee\n",
    "14 right knee\n",
    "15 left ankle\n",
    "16 right ankle\n",
    "\n",
    "\n",
    "JOINT_NAMES = [\n",
    "# 25 OpenPose joints (in the order provided by OpenPose)\n",
    "'OP Nose',\n",
    "'OP Neck',\n",
    "'OP RShoulder',\n",
    "'OP RElbow',\n",
    "'OP RWrist',\n",
    "'OP LShoulder',\n",
    "'OP LElbow',\n",
    "'OP LWrist',\n",
    "'OP MidHip',\n",
    "'OP RHip',\n",
    "'OP RKnee',\n",
    "'OP RAnkle',\n",
    "'OP LHip',\n",
    "'OP LKnee',\n",
    "'OP LAnkle',\n",
    "'OP REye',\n",
    "'OP LEye',\n",
    "'OP REar',\n",
    "'OP LEar',\n",
    "'OP LBigToe',\n",
    "'OP LSmallToe',\n",
    "'OP LHeel',\n",
    "'OP RBigToe',\n",
    "'OP RSmallToe',\n",
    "'OP RHeel',\n",
    "### MAP TO THE FOLLOWING:\n",
    "# 24 Ground Truth joints (superset of joints from different datasets)\n",
    "'0Right Ankle',\n",
    "'1Right Knee',\n",
    "'2Right Hip',\n",
    "'3Left Hip',\n",
    "'4Left Knee',\n",
    "'5Left Ankle',\n",
    "'6Right Wrist',\n",
    "'7Right Elbow',\n",
    "'8Right Shoulder',\n",
    "'9Left Shoulder',\n",
    "'10Left Elbow',\n",
    "'11Left Wrist',\n",
    "'12Neck (LSP)',\n",
    "'13Top of Head (LSP)',\n",
    "'14Pelvis (MPII)',\n",
    "'15Thorax (MPII)',\n",
    "'16Spine (H36M)',\n",
    "'17Jaw (H36M)',\n",
    "'18Head (H36M)',\n",
    "'19Nose',\n",
    "'20Left Eye',\n",
    "'21Right Eye',\n",
    "'22Left Ear',\n",
    "'23Right Ear'\n",
    "\n",
    "# Dict containing the joints in numerical order\n",
    "JOINT_IDS = {JOINT_NAMES[i]: i for i in range(len(JOINT_NAMES))}\n",
    "\n",
    "# Map joints to SMPL joints\n",
    "JOINT_MAP = {\n",
    "'OP Nose': 24, 'OP Neck': 12, 'OP RShoulder': 17,\n",
    "'OP RElbow': 19, 'OP RWrist': 21, 'OP LShoulder': 16,\n",
    "'OP LElbow': 18, 'OP LWrist': 20, 'OP MidHip': 0,\n",
    "'OP RHip': 2, 'OP RKnee': 5, 'OP RAnkle': 8,\n",
    "'OP LHip': 1, 'OP LKnee': 4, 'OP LAnkle': 7,\n",
    "'OP REye': 25, 'OP LEye': 26, 'OP REar': 27,\n",
    "'OP LEar': 28, 'OP LBigToe': 29, 'OP LSmallToe': 30,\n",
    "'OP LHeel': 31, 'OP RBigToe': 32, 'OP RSmallToe': 33, 'OP RHeel': 34,\n",
    "'Right Ankle': 8, 'Right Knee': 5, 'Right Hip': 45,\n",
    "'Left Hip': 46, 'Left Knee': 4, 'Left Ankle': 7,\n",
    "'Right Wrist': 21, 'Right Elbow': 19, 'Right Shoulder': 17,\n",
    "'Left Shoulder': 16, 'Left Elbow': 18, 'Left Wrist': 20,\n",
    "'Neck (LSP)': 47, 'Top of Head (LSP)': 48,\n",
    "'Pelvis (MPII)': 49, 'Thorax (MPII)': 50,\n",
    "'Spine (H36M)': 51, 'Jaw (H36M)': 52,\n",
    "'Head (H36M)': 53, 'Nose': 24, 'Left Eye': 26,\n",
    "'Right Eye': 25, 'Left Ear': 28, 'Right Ear': 27\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c446fd-64db-4fb5-b311-cbdbf2136e35",
   "metadata": {},
   "source": [
    "#### Create .npz file for WEARMAI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a260c0-a7b7-4d4d-8f2f-23aa66256db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully generated wearmai_train.npz\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_datasets.py --train_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768c25f-707c-485d-b70d-8789ea2d8b56",
   "metadata": {},
   "source": [
    "#### Create .npy data with initial fittings by running SMPLify once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d984bf2a-042b-4d12-ae8f-23ad0e6a7871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 7999\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "^C0%|                                       | 20/7999 [01:02<6:36:05,  2.98s/it]\n",
      "  0%|                                       | 20/7999 [01:03<7:03:23,  3.18s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/Desktop/SPIN/smplify/wearmai_fit.py\", line 91, in <module>\n",
      "    fit_wearmai()\n",
      "  File \"/home/simon/Desktop/SPIN/smplify/wearmai_fit.py\", line 73, in fit_wearmai\n",
      "    vertices, joints, pose, betas, cam_t, reprojection_loss = smplify(\n",
      "  File \"/home/simon/Desktop/SPIN/smplify/smplify.py\", line 82, in __call__\n",
      "    smpl_output = self.smpl(global_orient=global_orient,\n",
      "  File \"/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/simon/Desktop/SPIN/models/smpl.py\", line 23, in forward\n",
      "    smpl_output = super(SMPL, self).forward(*args, **kwargs)\n",
      "  File \"/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/smplx/body_models.py\", line 369, in forward\n",
      "    vertices, joints = lbs(betas, full_pose, self.v_template,\n",
      "  File \"/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/smplx/lbs.py\", line 218, in lbs\n",
      "    pose_feature = (rot_mats[:, 1:, :, :] - ident).view([batch_size, -1])\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python smplify/wearmai_fit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15119a-b747-43aa-a64c-06b17efd07d7",
   "metadata": {},
   "source": [
    "### Try initial round of training before modifying camera fov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e89b89-17de-4d92-ba9d-943d061c90d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] --name NAME [--time_to_run TIME_TO_RUN] [--resume]\n",
      "                [--num_workers NUM_WORKERS] [--pin_memory | --no_pin_memory]\n",
      "                [--log_dir LOG_DIR] [--checkpoint CHECKPOINT]\n",
      "                [--from_json FROM_JSON]\n",
      "                [--pretrained_checkpoint PRETRAINED_CHECKPOINT]\n",
      "                [--num_epochs NUM_EPOCHS] [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                [--summary_steps SUMMARY_STEPS] [--test_steps TEST_STEPS]\n",
      "                [--checkpoint_steps CHECKPOINT_STEPS] [--img_res IMG_RES]\n",
      "                [--rot_factor ROT_FACTOR] [--noise_factor NOISE_FACTOR]\n",
      "                [--scale_factor SCALE_FACTOR] [--ignore_3d]\n",
      "                [--shape_loss_weight SHAPE_LOSS_WEIGHT]\n",
      "                [--keypoint_loss_weight KEYPOINT_LOSS_WEIGHT]\n",
      "                [--pose_loss_weight POSE_LOSS_WEIGHT]\n",
      "                [--beta_loss_weight BETA_LOSS_WEIGHT]\n",
      "                [--openpose_train_weight OPENPOSE_TRAIN_WEIGHT]\n",
      "                [--gt_train_weight GT_TRAIN_WEIGHT] [--run_smplify]\n",
      "                [--smplify_threshold SMPLIFY_THRESHOLD]\n",
      "                [--num_smplify_iters NUM_SMPLIFY_ITERS]\n",
      "                [--shuffle_train | --no_shuffle_train]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "\n",
      "Required:\n",
      "  --name NAME           Name of the experiment\n",
      "\n",
      "General:\n",
      "  --time_to_run TIME_TO_RUN\n",
      "                        Total time to run in seconds. Used for training in\n",
      "                        environments with timing constraints\n",
      "  --resume              Resume from checkpoint (Use latest checkpoint by\n",
      "                        default\n",
      "  --num_workers NUM_WORKERS\n",
      "                        Number of processes used for data loading\n",
      "  --pin_memory\n",
      "  --no_pin_memory\n",
      "\n",
      "io:\n",
      "  --log_dir LOG_DIR     Directory to store logs\n",
      "  --checkpoint CHECKPOINT\n",
      "                        Path to checkpoint\n",
      "  --from_json FROM_JSON\n",
      "                        Load options from json file instead of the command\n",
      "                        line\n",
      "  --pretrained_checkpoint PRETRAINED_CHECKPOINT\n",
      "                        Load a pretrained checkpoint at the beginning training\n",
      "\n",
      "Training Options:\n",
      "  --num_epochs NUM_EPOCHS\n",
      "                        Total number of training epochs\n",
      "  --lr LR               Learning rate\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size\n",
      "  --summary_steps SUMMARY_STEPS\n",
      "                        Summary saving frequency\n",
      "  --test_steps TEST_STEPS\n",
      "                        Testing frequency during training\n",
      "  --checkpoint_steps CHECKPOINT_STEPS\n",
      "                        Checkpoint saving frequency\n",
      "  --img_res IMG_RES     Rescale bounding boxes to size [img_res, img_res]\n",
      "                        before feeding them in the network\n",
      "  --rot_factor ROT_FACTOR\n",
      "                        Random rotation in the range [-rot_factor, rot_factor]\n",
      "  --noise_factor NOISE_FACTOR\n",
      "                        Randomly multiply pixel values with factor in the\n",
      "                        range [1-noise_factor, 1+noise_factor]\n",
      "  --scale_factor SCALE_FACTOR\n",
      "                        Rescale bounding boxes by a factor of\n",
      "                        [1-scale_factor,1+scale_factor]\n",
      "  --ignore_3d           Ignore GT 3D data (for unpaired experiments\n",
      "  --shape_loss_weight SHAPE_LOSS_WEIGHT\n",
      "                        Weight of per-vertex loss\n",
      "  --keypoint_loss_weight KEYPOINT_LOSS_WEIGHT\n",
      "                        Weight of 2D and 3D keypoint loss\n",
      "  --pose_loss_weight POSE_LOSS_WEIGHT\n",
      "                        Weight of SMPL pose loss\n",
      "  --beta_loss_weight BETA_LOSS_WEIGHT\n",
      "                        Weight of SMPL betas loss\n",
      "  --openpose_train_weight OPENPOSE_TRAIN_WEIGHT\n",
      "                        Weight for OpenPose keypoints during training\n",
      "  --gt_train_weight GT_TRAIN_WEIGHT\n",
      "                        Weight for GT keypoints during training\n",
      "  --run_smplify         Run SMPLify during training\n",
      "  --smplify_threshold SMPLIFY_THRESHOLD\n",
      "                        Threshold for ignoring SMPLify fits during training\n",
      "  --num_smplify_iters NUM_SMPLIFY_ITERS\n",
      "                        Number of SMPLify iterations\n",
      "  --shuffle_train       Shuffle training data\n",
      "  --no_shuffle_train    Don't shuffle training data\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a10b02c2-3c1a-42d4-b5f3-7e0ffb1779eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints: (7999, 49, 3)\n",
      "/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
      "/home/simon/Desktop/SPIN/utils/base_trainer.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_file)\n",
      "Checkpoint loaded\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/Desktop/SPIN/train/fits_dict.py\", line 22, in __init__\n",
      "    self.fits_dict[ds_name] = torch.from_numpy(np.load(dict_file))\n",
      "  File \"/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py\", line 451, in load\n",
      "    fid = stack.enter_context(open(os.fspath(file), \"rb\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/simon/Desktop/SPIN/logs/train_wearmai_data_test1/checkpoints/wearmai_fits.npy'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/simon/Desktop/SPIN/train.py\", line 6, in <module>\n",
      "    trainer = Trainer(options)\n",
      "  File \"/home/simon/Desktop/SPIN/utils/base_trainer.py\", line 21, in __init__\n",
      "    self.init_fn()\n",
      "  File \"/home/simon/Desktop/SPIN/train/trainer.py\", line 48, in init_fn\n",
      "    self.fits_dict = FitsDict(self.options, self.train_ds)\n",
      "  File \"/home/simon/Desktop/SPIN/train/fits_dict.py\", line 26, in __init__\n",
      "    self.fits_dict[ds_name] = torch.from_numpy(np.load(dict_file))\n",
      "  File \"/home/simon/Desktop/SPINenv/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py\", line 451, in load\n",
      "    fid = stack.enter_context(open(os.fspath(file), \"rb\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/static_fits/wearmai_fits.npy'\n"
     ]
    }
   ],
   "source": [
    "#Train on just WEARMAI data \n",
    "#Batch size 64--> GPU memory error\n",
    "#Change mixed_dataset.py to only look for WEARMAI\n",
    "!python3 train.py --name train_wearmai_data_test1 \\\n",
    "                  --pretrained_checkpoint=data/model_checkpoint.pt \\\n",
    "                  --run_smplify \\\n",
    "                  --num_epochs 10 \\\n",
    "                  --batch_size 32 \\\n",
    "                  --num_workers 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c2800-e5e5-4d01-9ea9-4b6ad7b4935d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
